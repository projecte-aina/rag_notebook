{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28f0531",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/projecte-aina/rag_notebook/blob/main/RAGDemo_LlamaIndex_Flor6.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05",
   "metadata": {
    "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05"
   },
   "source": [
    "# Retrieval Augmented Generation demo using Flor6.3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc195b8",
   "metadata": {
    "id": "0bc195b8"
   },
   "source": [
    "This notebook shows how to create a basic Retrieval Augmented Generation (RAG) system for Catalan using a QA-optimized model based on the Flor6.3b foundational model created for the AINA project. [FlorQARAG](https://huggingface.co/projecte-aina/FlorQARAG) \n",
    "This demo should run with the Tesla instance provided free of charge in Google Colab. You can create a copy of this model and use another pdf document to interrogate, if you wish.\n",
    "For a more in-depth description of RAG, go to this blog from IBM research: [\"What is retrieval-augmented generation?\"](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)\n",
    "First step: install all the libraries you are going to need:\n",
    "If you're opening this Notebook on Colab, you will probably need to install LlamaIndex ðŸ¦™. (Based on an [original tutorial](https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb) by the LlamaIndex Team)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c49377",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03c49377",
    "outputId": "ed5b6bcc-67e6-48bc-c24c-16b1a18e4985"
   },
   "outputs": [],
   "source": [
    "%pip install huggingface_hub llama-index-embeddings-huggingface llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296c135",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5296c135",
    "outputId": "1b29457f-1986-4537-ec8d-738970a84f25"
   },
   "outputs": [],
   "source": [
    "!pip install llama-index langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bcec1",
   "metadata": {},
   "source": [
    "We now need to implement the embedding model we will use in order to convert the documents into searchable vectors, and to select the vector store that will allow us to store and retrieve them. We will be using a small, multilingual one from the Huggingface [infloat](https://huggingface.co/intfloat/multilingual-e5-small) repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7",
    "outputId": "a173fdf0-cb5a-431d-fc44-e349f34fd7b2"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from llama_index.core import Settings\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"intfloat/multilingual-e5-small\",\n",
    "    embed_batch_size=2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca8fc2",
   "metadata": {
    "id": "36ca8fc2"
   },
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0bd6c",
   "metadata": {},
   "source": [
    "The document we'll use for the demo is the White Paper describing the Generalitat de Catalunya AI Strategy, a 77 page document in catalan downloadable from this site: [link](https://politiquesdigitals.gencat.cat/ca/economia/catalonia-ai). You can use another pdf, if it is not a scanned version and can be converted into plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaab3b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "edaab3b0",
    "outputId": "104f55a1-7b27-48bf-d35a-d0f40dc7a5bb"
   },
   "outputs": [],
   "source": [
    "!mkdir -p 'data/estrategia/'\n",
    "!wget 'https://politiquesdigitals.gencat.cat/web/.content/00-arbre/economia/catalonia-ai/Estrategia_IA_Catalunya_VFinal_CAT.pdf' -O 'data/estrategia/Estrategia_IA_Catalunya.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119",
   "metadata": {
    "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119"
   },
   "source": [
    "#### Load documents, build the VectorStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84732a5f",
   "metadata": {},
   "source": [
    "The next step is to convert the document into text, and store it in a vector store index. The important parameters are the size of the searchable chunks to divide the document into, as well as the overlap of the chunks so you always have a relevant context around the sentences you are going to use to search the answers to your questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a895ac-2e8b-4d55-97bd-ad614dceda40",
   "metadata": {
    "id": "90a895ac-2e8b-4d55-97bd-ad614dceda40"
   },
   "outputs": [],
   "source": [
    "# load documents, creates one document per page from the PDF\n",
    "documents = SimpleDirectoryReader(\"./data/estrategia/\").load_data()\n",
    "\n",
    "#index documents segmented into chunks of max. 512 tokens\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 100\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YYAh5Eqtx_uF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYAh5Eqtx_uF",
    "outputId": "94ae1e8d-f984-4c02-eb0f-26a4c49fcfc5"
   },
   "outputs": [],
   "source": [
    "#check one of the indexed documents/pages (page 1)\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FFTvrQZg3pDv",
   "metadata": {
    "id": "FFTvrQZg3pDv"
   },
   "source": [
    "#### Set up LLM model and prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18703fd",
   "metadata": {},
   "source": [
    "Now you need to set up the Large Language Model you will be using to look for the information and generate an answer, in our case a RAG-optimized version of Flor6.3b from the projecte-aina repo in Huggingface. This model has been fine-tuned using an istruction set that requires a prompt template that provides a question (\"instruction\"), a searchable context (that is going to be provided by the query engine based on the similarities of the embedings with the stored chunks), and will generate an answer based on these elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1726fff",
   "metadata": {
    "id": "b1726fff"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# dolly format prompt template\n",
    "query_prompt = PromptTemplate(\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Context\\n{context_str}### Answer:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40a357-8a8f-405d-b355-e0caf23bee3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9db7ff6d01354e05a43db79dcd170e1c",
      "f86d0ce59e7e43e68569c46b348f2b00",
      "09b41d640bfd441da1df05a665fad0c0",
      "75453fba4bc2463998a77a34a129e0fe",
      "56a4f59c65c34fed93fb966c1eac36f5",
      "e38c42e8ab344aafa992d7674ca45d73",
      "5afc05397e3a414b8b2bd5e53f33cc8c",
      "7fe9b1537cf5417a92c1e95d7938aa08",
      "b81b6da16ed6492189523680afdc2351",
      "3265be36ea7744998268fb785d03c9ac",
      "7fe661f08b0947de93bbc3a9f3da9177"
     ]
    },
    "id": "6a40a357-8a8f-405d-b355-e0caf23bee3c",
    "outputId": "2abd890e-ba40-464c-e9bc-d1c12f8f40b6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.25, \"do_sample\": True},\n",
    "    tokenizer_name=\"projecte-aina/FlorQARAG\",\n",
    "    model_name=\"projecte-aina/FlorQARAG\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4",
   "metadata": {
    "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4"
   },
   "source": [
    "#### Query Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbe66b",
   "metadata": {},
   "source": [
    "Once we have this setup, we can pass on our query to the query engine using the correct prompt format, and generate a response that will be based on the selected chunks from the document that have the highest probability of conveying the correct and relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
    "outputId": "e47f1f24-f081-40c1-f441-0c19ba048e79"
   },
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "#set prompt template to use Dolly format\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": query_prompt}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"QuÃ© Ã©s lâ€™EstratÃ¨gia dâ€™intelÂ·ligÃ¨ncia artificial de Catalunya?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabafc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c42733dd",
    "outputId": "c1946ff5-4759-444e-8294-3052344e692c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b282580",
   "metadata": {
    "id": "6b282580"
   },
   "source": [
    "#### Query Index - Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313544bc",
   "metadata": {
    "id": "313544bc"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=True)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": query_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5ab85-25e4-4460-8b6a-3c119d92ba48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1c5ab85-25e4-4460-8b6a-3c119d92ba48",
    "outputId": "8dff7288-18b6-438d-9cdf-11cbe9a7aaab"
   },
   "outputs": [],
   "source": [
    "response_stream = query_engine.query(\"En Europa, quin paÃ­s Ã©s capdavanter en intelÂ·ligÃ¨ncia artificial?\")\n",
    "response_stream.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09b41d640bfd441da1df05a665fad0c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fe9b1537cf5417a92c1e95d7938aa08",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b81b6da16ed6492189523680afdc2351",
      "value": 2
     }
    },
    "3265be36ea7744998268fb785d03c9ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56a4f59c65c34fed93fb966c1eac36f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5afc05397e3a414b8b2bd5e53f33cc8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75453fba4bc2463998a77a34a129e0fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3265be36ea7744998268fb785d03c9ac",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7fe661f08b0947de93bbc3a9f3da9177",
      "value": "â€‡2/2â€‡[01:02&lt;00:00,â€‡27.80s/it]"
     }
    },
    "7fe661f08b0947de93bbc3a9f3da9177": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fe9b1537cf5417a92c1e95d7938aa08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9db7ff6d01354e05a43db79dcd170e1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f86d0ce59e7e43e68569c46b348f2b00",
       "IPY_MODEL_09b41d640bfd441da1df05a665fad0c0",
       "IPY_MODEL_75453fba4bc2463998a77a34a129e0fe"
      ],
      "layout": "IPY_MODEL_56a4f59c65c34fed93fb966c1eac36f5"
     }
    },
    "b81b6da16ed6492189523680afdc2351": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e38c42e8ab344aafa992d7674ca45d73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f86d0ce59e7e43e68569c46b348f2b00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e38c42e8ab344aafa992d7674ca45d73",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5afc05397e3a414b8b2bd5e53f33cc8c",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
